{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1c32a1",
   "metadata": {},
   "source": [
    "#### **Named Entity Recognition (NER) with George Elliot's *Mill on the Floss***\n",
    "\n Bronwyn Bowles-King"
    "\n bronwynbowlesking@gmail.com"
    "##### Step 1: Preparation - Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93fad166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_lg') # Load the large NLP model\n",
    "nlp.max_length = 1200000 # Increase max length to fit the text and avoid errors later\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9359524",
   "metadata": {},
   "source": [
    "##### Step 2: Preprocess the text\n",
    "\n",
    "Clean the text and remove punctuation, numbers, symbols and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning function\n",
    "def clean_text(text):\n",
    "    \n",
    "    if text is None:\n",
    "        raise ValueError(\"Input text is None.\")\n",
    "    \n",
    "    # Replace newline, carriage return, and tab characters with a space\n",
    "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "\n",
    "    # Normalize text to remove encoding artifacts\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Replace problematic encoding artifacts\n",
    "    text = text.replace('â', '').replace('€', '').replace('_', '').replace('™', '').replace('œ', '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Advanced cleaning function\n",
    "def clean_text_advanced(text):\n",
    "    \"\"\"\n",
    "    Further cleans the text by:\n",
    "    - Removing possessive \"'s\"\n",
    "    - Removing punctuation, symbols, and numbers\n",
    "    - Removing stopwords\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        raise ValueError(\"Input text is None in clean_text_advanced function.\")\n",
    "    \n",
    "    # Remove possessive \"'s\"\n",
    "    text = re.sub(r\"\\b(\\w+)'s\\b\", r\"\\1\", text)\n",
    "    \n",
    "   # Remove some punctuation and symbols except end punctuation\n",
    "    text = re.sub(r\"[^\\w\\s.!?]\", \" \", text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    \n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Remove stopwords and extra spaces\n",
    "    cleaned_tokens = [token.text for token in doc if token.text.lower() not in STOP_WORDS]\n",
    "    cleaned_text = \" \".join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Process the text file\n",
    "def process_file(file_path, output_file_path=None):\n",
    "    \"\"\"\n",
    "    Reads, cleans, and saves the content of a text file.\n",
    "    If output_file_path is provided, saves the cleaned text to a new file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"File read successfully: {file_path}\")\n",
    "        \n",
    "        # Run basic cleaning\n",
    "        text = clean_text(text)\n",
    "        print(\"Basic cleaning completed.\")\n",
    "        \n",
    "        # Run advanced cleaning\n",
    "        text = clean_text_advanced(text)\n",
    "        print(\"Advanced cleaning completed.\")\n",
    "        \n",
    "        # Save cleaned text\n",
    "        save_path = output_file_path if output_file_path else file_path\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"Text cleaned and saved successfully: {save_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "\n",
    "\n",
    "# Input file path\n",
    "# file_path = r'your/file/pathway/The_Mill_on_the_Floss.txt'\n",
    "\n",
    "# Output file path\n",
    "# output_file_path = r'your/file/pathway/The_Mill_on_the_Floss_cleaned.txt'\n",
    "\n",
    "\n",
    "# Process the file and save with a new name\n",
    "process_file(file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3ce00",
   "metadata": {},
   "source": [
    "##### Step 3: Split the text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed185b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure sentenciser with a suitable max_length for the text\n",
    "sentencizer = English()\n",
    "sentencizer.max_length = 1_200_000  \n",
    "sentencizer.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Read the cleaned text content from the file\n",
    "with open(output_file_path, 'r', encoding='utf-8') as f:\n",
    "\ttext = f.read()\n",
    "\n",
    "# Split text into sentences\n",
    "sentences = [sent.text for sent in sentencizer(text).sents]\n",
    "\n",
    "# Print first 10 sentences to see results\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71040d",
   "metadata": {},
   "source": [
    "##### Step 4: Load and run the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text to remove encoding artifacts\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKD', text)\n",
    "\n",
    "# Process NER and save results\n",
    "# with open('your/file/pathway/ner_results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Sentence', 'Entity', 'Label'])\n",
    "    \n",
    "    for doc in nlp.pipe(sentences, batch_size=50):  # Process sentences in batches\n",
    "        for ent in doc.ents:\n",
    "            # Normalise text before writing\n",
    "            writer.writerow([normalize_text(doc.text), normalize_text(ent.text), ent.label_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dbd47",
   "metadata": {},
   "source": [
    "##### Step 5: Extract character names and their frequency and display on a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34655201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('your/file/pathway/ner_results.csv') \n",
    "\n",
    "character_counts = df[df['Label'] == 'PERSON']['Entity'].value_counts()\n",
    "\n",
    "character_counts = character_counts.reset_index() # Convert the results to a dataframe and reset index\n",
    "\n",
    "character_counts.columns = ['Name', 'Count']  # Rename the columns\n",
    "\n",
    "# Show the top 10 characters\n",
    "print(character_counts.head(10))  \n",
    "\n",
    "# Save to CSV\n",
    "# character_counts.to_csv('your/file/pathway/character_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdf431",
   "metadata": {},
   "source": [
    "##### Step 6: Plot of character names with *seaborn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1fbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load character counts\n",
    "# character_counts = pd.read_csv('your/file/pathway/character_counts.csv')\n",
    "\n",
    "# Get top 15 characters\n",
    "top_characters = character_counts.nlargest(15, 'Count')\n",
    "\n",
    "# Set the plot size and aesthetic style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x='Name', y='Count', data=top_characters, palette='colorblind')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)  # Adjust fontsize as needed\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Top 15 characters in The Mill on the Floss', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Character name', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the plot to a file in the interface or with:\n",
    "# plt.savefig('your/file/pathway/top_characters.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd86e9a",
   "metadata": {},
   "source": [
    "##### Step 7: Analyse character relationships\n",
    "\n",
    "**Determine the most frequent character pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_entities = df.groupby('Sentence')['Entity'].apply(list)\n",
    "\n",
    "# Find characters mentioned together\n",
    "co_occurrences = []\n",
    "for entities in sentence_entities:\n",
    "    if len(entities) >= 2:\n",
    "        co_occurrences.extend(combinations(set(entities), 2))\n",
    "\n",
    "# Show the 15 most common character pairs\n",
    "print(\"15 most frequent character pairs:\")\n",
    "print(Counter(co_occurrences).most_common(15))\n",
    "\n",
    "# Convert all pairings to a DataFrame for saving\n",
    "co_occurrences_df = pd.DataFrame(Counter(co_occurrences).most_common(15), columns=['Pair', 'Count'])\n",
    "\n",
    "# Save all co_occurrences to CSV\n",
    "# co_occurrences_df.to_csv('your/file/pathway/character_co_occurrences_top_15_pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c5dce",
   "metadata": {},
   "source": [
    "**Determine the most frequent character triads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bf618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_entities = df.groupby('Sentence')['Entity'].apply(list)\n",
    "\n",
    "# Find groups of three characters mentioned together\n",
    "co_occurrences_3 = []\n",
    "for entities in sentence_entities:\n",
    "    if len(entities) >= 3:\n",
    "        co_occurrences_3.extend(combinations(set(entities), 3))\n",
    "\n",
    "# Show the 15 most common triads\n",
    "print(\"15 most frequent character triads:\")\n",
    "print(Counter(co_occurrences_3).most_common(5))\n",
    "\n",
    "# Convert all co_occurrences to a DataFrame for saving\n",
    "co_occurrences_3_df = pd.DataFrame(Counter(co_occurrences_3).most_common(15), columns=['Triad', 'Count'])\n",
    "\n",
    "# Save all co_occurrences to CSV\n",
    "# co_occurrences_3_df.to_csv('your/file/pathway/character_co_occurrences_top_15_triads.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03872a0",
   "metadata": {},
   "source": [
    "##### Step 8: Create a network diagram of main character relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of character names\n",
    "target_characters = [\"Maggie\", \"Tom\", \"Lucy\", \"Philip\", \"Stephen\", \"Bob\"]  \n",
    "\n",
    "# Filter co-occurrences to exclude non-character names\n",
    "filtered_co_occurrences = [\n",
    "    pair for pair in co_occurrences if pair[0] in target_characters and pair[1] in target_characters\n",
    "]\n",
    "\n",
    "# Get the most common pairings from the filtered list\n",
    "most_common_filtered = Counter(filtered_co_occurrences).most_common(30)\n",
    "\n",
    "# Convert to DataFrame and save co_occurrences to CSV\n",
    "filtered_co_occurrences_df = pd.DataFrame(most_common_filtered, columns=['Pair', 'Count'])\n",
    "# filtered_co_occurrences_df.to_csv('your/file/pathway/filtered_character_co_occurrences_top_30_pairs.csv', index=False)\n",
    "\n",
    "\n",
    "# Create a graph from the filtered co-occurrences\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges and weights from the filtered co-occurrences\n",
    "for pair, count in Counter(filtered_co_occurrences).items():\n",
    "    G.add_edge(pair[0], pair[1], weight=count)\n",
    "\n",
    "# Define a color mapping for each character\n",
    "color_mapping = {\n",
    "    \"Maggie\": \"#F5B599\",\n",
    "    \"Tom\": \"#ADCBEA\",\n",
    "    \"Lucy\": \"#CBC3E3\",\n",
    "    \"Philip\": \"#99DDFF\",\n",
    "    \"Stephen\": \"#FFD580\",\n",
    "    \"Bob\": \"#ABF1BC\",\n",
    "}\n",
    "\n",
    "# Check if the graph has nodes and edges\n",
    "if G.number_of_nodes() == 0 or G.number_of_edges() == 0:\n",
    "    print(\"No nodes or edges in the graph.\")\n",
    "else:\n",
    "    # Draw the network graph\n",
    "    plt.figure(figsize=(12, 12))  # Increase figure size for better resolution\n",
    "    \n",
    "    # Use a spring layout for better spacing\n",
    "    pos = nx.spring_layout(G, k=0.33)  # Adjust k for spacing\n",
    "    node_colors = [color_mapping[node] for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=1200, node_color=node_colors)  # Increase node size\n",
    "    nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.7)  # Increase edge width\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10)  # Adjust font size for labels\n",
    "    \n",
    "    # Add edge labels for frequencies with smaller font size and adjust position\n",
    "    edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, verticalalignment='bottom')\n",
    "    \n",
    "    # Add titles\n",
    "    plt.title('Character co-occurrence network', fontsize=16, fontweight='bold')  # Increase title font size\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    \n",
    "    # Save the plot with higher DPI\n",
    "    # plt.savefig('your/file/pathway/character_network.png', dpi=300, bbox_inches='tight') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5345c6ee",
   "metadata": {},
   "source": [
    "##### **Sources**\n",
    "\n",
    "Cooke, G. W. (2008 [1884]). *George Eliot: A Critical Study of Her Life, Writings and Philosophy.* Boston: James R. Esgood & Co. https://www.gutenberg.org/ebooks/11680\n",
    "  \n",
    "Eliot, G. (2004 [1860]). *The Mill on the Floss.*  Project Gutenberg. https://www.gutenberg.org/ebooks/6688\n",
    "\n",
    "Esty, J.D. (2002). Nationhood, Adulthood, and the Ruptures of *Bildung*: Arresting Development in *The Mill on the Floss*. In: Yousaf, N. & Maunder, A. (Eds), *The Mill on the Floss and Silas Marner*. London: Palgrave.\n",
    "\n",
    "Frerebeau, N. (2025). Paul Tol's Color Schemes. https://cran.r-project.org/web/packages/khroma/vignettes/tol.html\n",
    "\n",
    "NetworkX. (2024). Introduction. https://networkx.org/documentation/stable/reference/introduction.html\n",
    "\n",
    "Makurath, P. A. (1975). The symbolism of the flood in Eliot's *The Mill on the Floss. Studies in the Novel*, 7(2), 298–300. http://www.jstor.org/stable/29531723\n",
    "\n",
    "Narozniak, R. (2019). Mapping the Movement of Desire in George Eliot: A Spatial Study of Character Itinerancy in *Adam Bede* and *The Mill on the Floss.* Master's thesis, George Washington University. https://www.proquest.com/openview/a39ea085397a37867f374d14e804b350\n",
    "\n",
    "Nayebpour, K. (2018). *Fictional minds and interpersonal relationships in George Eliot's* The Mill on the Floss. Newcastle upon Tyne, UK: Cambridge Scholars Publishing. \n",
    "\n",
    "spaCy. (2025a). Available trained pipelines for English. https://spacy.io/models/en\n",
    "\n",
    "spaCy. (2025b). Sentencizer. https://spacy.io/api/sentencizer\n",
    "\n",
    "Wiesenfarth, J. (1976). Legend in *The Mill on the Floss. Texas Studies in Literature and Language*, 18(1), 20–41. http://www.jstor.org/stable/40754426\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
